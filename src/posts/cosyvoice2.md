---
title: '【论文分享】｜ cosyvoice语音合成论文分享'
date: 2025-08-12
author:
   name: 张靖鸿
category: insights
---
## CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens

论文链接：[https://arxiv.org/pdf/2407.05407](https://arxiv.org/pdf/2407.05407)
项目链接：[https://github.com/FunAudioLLM/CosyVoice](https://github.com/FunAudioLLM/CosyVoice)

## 速览
CosyVoice 1 是阿里通义实验室于 2023 年推出的多语种语音生成模型，核心目标为 ​​实现零样本语音克隆并提升语义一致性​​。其采用监督式语义 Token 提取（Whisper Encoder + 向量量化 VQ），通过小型 Token Predictor 与自定义 Encoder 架构生成语音。然而，其 Tokenizer 存在利用率低、冗余高的问题，且仅支持​​非流式离线生成​​，导致在实时场景中面临显著延迟瓶颈。

1. 模型规模与训练​​

    * 基于​​万小时级多语种数据​​（9 种语言）训练，支持中、英、日等语言的零样本克隆。

2. ​​性能评测​​
    * ​​语义一致性​​：在 SEED-TTS 评测集上表现优异，内容一致性超越传统模型，接近人类水平；

    * ​​延迟表现​​：因缺乏流式支持，端到端延迟显著高于后续版本（如 CosyVoice 2 首包延迟优化至 150ms）；

    * ​​错误率​​：CER/WER 高于后续版本，CosyVoice 2/3 通过架构改进进一步降低 30%-50%

## 摘要
1. CosyVoice1 主要针对传统 TTS 模型在​​实时交互​​与​​细粒度控制​​上的局限性进行优化。传统模型如 FastSpeech 虽通过非自回归架构提升推理速度，但无法支持流式生成，导致首包延迟高（通常 >500ms），难以满足实时语音交互需求，而其他端到端模型VITS 虽通过端到端建模提升自然度，但对多语言混合、方言及情感表达的控制依赖额外模块（如 Emo-VITS），灵活性不足CosyVoice1 通过​​双向流式建模​​实现逐句生成，首包延迟压降至 150ms，同时引入​​自然语言指令控制​​（如方言切换、情感标记），用户仅需输入文本指令（例如“用四川话说”）即可动态调整语音风格，无需重新训练或复杂参数配置，显著降低实时场景的应用门槛。

2. 较于传统模型，CosyVoice1 的优势集中于​​多语言泛化​​与​​资源效率​​。其采用​​监督式语义标记技术​​（Whisper + 矢量量化），从多语种 ASR 数据中提取强语义表征，支持中、英、日、韩及粤语等方言的无缝切换，音色相似度达 90%+，显著优于 VITS 在多语言场景下的适配成本

3. 在资源利用上，CosyVoice1 结合 ​​TensorRT 加速与动态批处理​​，RTF（实时因子）稳定在 0.2 以下，同等硬件下较 FastSpeech 提速 40%，且支持 6GB 显存设备部署，而 VITS 通常需 16GB 显存以上

## 模型结构框架
# 1. ​​文本编码器（Text Encoder）​​
* ​​功能​​：将输入文本映射到与语音语义对齐的隐空间向量，支持多语言文本（如中、英、日、韩）。
* ​​技术实现​​：
    * 基于BERT等预训练语言模型提取上下文语义。
    * 融合韵律标记（如停顿、重音）增强语音自然度。

* ​​创新点​​：无需强制对齐器或音素转换模块，直接生成富文本表征。

# 2. ​​语音tokenizer（Speech Tokenizer）​​
* ​​功能​​：生成​​监督语义token（S³ tokens）​​，作为连接文本与声学的中间表示。
* ​​技术实现​​：
    * 在多语言ASR模型（如Paraformer）的编码器后插入​​矢量量化层（VQ-VAE）​​，将语音特征离散化为token。
    * 监督训练确保标记显式捕获语义信息（如内容一致性），区别于无监督编解码器（如EnCodec）。

* ​​创新点​​：标记与文本强对齐，提升零样本克隆的内容保真度。

# 3. ​​大语言模型（Text-to-Token LLM）​​
* ​​功能​​：以自回归方式将文本编码器输出解码为S³ token序列，重构语音语义流。
* ​​技术实现​​：
    * 基于GPT架构的自回归Transformer，输入为文本向量+历史token，预测下一token。
    * 支持参考音频特征注入（如说话人嵌入），实现音色克隆。

* ​​创新点​​：将TTS任务转化为序列生成问题，简化流程并支持长上下文建模。

# 4. ​​条件流匹配模型（Conditional Flow Matching）​​
* ​​功能​​：将S³ token转换为梅尔频谱图，融入音色与情感控制。
* ​​技术实现​​：
    * 基于常微分方程（ODE）的扩散模型，学习从噪声到目标频谱的确定性路径。
    * 输入包括：S³ token序列、说话人特征（x-vector）、情感标签（可选）。

* ​​创新点​​：避免随机扩散的不稳定性，提升合成效率与音色相似度。

# 5. ​​声码器（Vocoder）​​
* ​​功能​​：将梅尔频谱图转换为最终语音波形。
* ​​技术实现​​：采用预训练的HiFiGAN，支持48kHz高保真输出。